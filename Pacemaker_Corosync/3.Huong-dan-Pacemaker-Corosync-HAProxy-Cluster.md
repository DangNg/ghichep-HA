## Hướng dẫn cấu hình HAProxy Cluster bằng Pacemaker + Corosync

### Menu

[1. Chuẩn bị] (#1)

[2. Các bước thực hiện] (#2)

- [2.1 Cài đặt Apache Web-server] (#2.1)
- [2.2 Cài đặt và cấu hình HAProxy] (#2.2)
- [2.3 Cài đặt Pacemaker và Corosync] (#2.3)
- [2.4 Cài đặt DRDB] (#2.4)
- [2.5 Cấu hình DRDB] (#2.5)
- [2.6 Khởi động DRDB meta-data storage] (#2.6)
- [2.7 Xác định DRDB Primary node] (#2.7)
- [2.8 Tạo và mount filesystem thiết bị DRDB] (#2.8)
- [2.9 Tạo cluster resource và đặt các thuộc tính] (#2.9)
- [2.10 Test trường hợp fail-over bằng tay] (#2.10)

[3. Tham khảo] (#3)

## 1. Chuẩn bị <a name="1" />

- 2 server sử dụng OS CentOS
- 2 ổ cứng có cùng dung lượng được gắn vào các node
- Cấu hình hostname cho các server
- Mở port 7788 trên các server

Cụ thể:

**Node 1**

```
OS: CentOS 7 64 bit
Device: /dev/sdb - 8GB
Hostname: node1
IP: 192.168.100.196
Gateway: 192.168.100.1
Network: 192.168.100.0/24
```

**Node 2**

```
OS: CentOS 7 64 bit
Device: /dev/sdb - 8GB
Hostname: node2
IP: 192.168.100.197
Gateway: 192.168.100.1
Network: 192.168.100.0/24
```

#### Mô hình

<img src="http://image.prntscr.com/image/2378e8f54640479aaf9bab185c69aa76.png" width=75% />

- Trước khi cài đặt, chúng ta phải cấu hình hostname cho mỗi node và ghi chúng vào `hosts`

```
[root@node1 ~] hostnamectl set-hostname node1
```

```
[root@node2 ~] hostnamectl set-hostname node2
```

- Ghi thêm vào `hosts` của mỗi server

```
vi /etc/hosts
```

```
[...]
192.168.100.196 node1
192.168.100.197 node2
```

## 2. Các bước thực hiện <a name="2" />

### 2.1 Cài đặt Apache Web-server <a name="2.1" />

Cài đặt Apache trên cả 2 node:

```
yum -y install httpd*
```

Cấu hình tường lửa cho phép người dùng bên ngoài có thể truy cập vào qua giao thức `http`:

```
firewall-cmd --permanent --add-service=http
firewall-cmd --reload
```

### 2.2 Cài đặt Pacemaker và Corosync <a name="2.2" />

Cài đặt các gói cluster trên cả 2 node bằng `yum`:

```
yum install -y pacemaker pcs fence-agents-all psmisc policycoreutils-python
```

Cũng như phần trên, chúng ta cấu hình tường lửa cho phép dịch vụ HA:

```
firewall-cmd --permanent --add-service=high-availability
firewall-cmd --reload
```

### 2.3 Cấu hình Cluster với Pacemaker và Corosync <a name="2.3" />

Trên cả 2 node, chúng ta đặt password cho user `hacluster` để xác thực với nhau, 2 mật khẩu phải trùng khớp

```
echo "redhat" | passwd --stdin hacluster
```

Tiếp theo, chúng ta khởi động dịch vụ trên cả 2 node:

```
systemctl start pcsd
systemctl enable pcsd
```

Trên node1, chúng ta xác thực 2 node với nhau bằng lệnh:

```
[root@node1 ~]# pcs cluster auth node1 node2 -u hacluster -p redhat
node1: Authorized
node2: Authorized
```

Sau khi xác thực, chúng ta tạo 1 cluster trên node 1 có tên là `mycluster` để chúng có thể tạo và đồng bộ các file cấu hình với nhau.

```
[root@node1 ~]# pcs cluster setup --name mycluster node1 node2
Shutting down pacemaker/corosync services...
Redirecting to /bin/systemctl stop  pacemaker.service
Redirecting to /bin/systemctl stop  corosync.service
Killing any remaining services...
Removing all cluster configuration files...
node1: Succeeded
node2: Succeeded
Synchronizing pcsd certificates on nodes node1, node2...
node1: Success
node2: Success
Restaring pcsd on the nodes in order to reload the certificates...
node1: Success
node2: Success
```

Khởi động và kích hoạt cluster mới tạo trên node 1 bằng lệnh:

```
[root@node1 ~]# pcs cluster start --all
[root@node1 ~]# pcs cluster enable --all
```

Xem lại trạng thái của cluster trên các node:

```
pcs status
```

Tắt Quorum và STONITH

```
pcs property set stonith-enabled=false
pcs property set no-quorum-policy=ignore
pcs property set default-resource-stickiness="INFINITY"
```

### 2.4 Cài đặt DRDB <a name="2.4" />

Để cài đặt DRDB mới nhất, chúng ta thêm repo của DRDB (các thao tác này làm trên cả 2 node):

```
rpm -ivh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm
```

Thêm Public key cho DRDB

```
rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-elrepo.org
```

Tiếp theo, chúng ta cài đặt DRDB trên cả 2 node:

```
yum -y install drbd*-utils kmod-drbd*
```

Nếu quá trình cài đặt trên không thành công hoặc xảy ra lỗi hãy chèn lại key và tiếp tục cài đặt:

```
rpm --import /etc/pki/rpm-gpg/*
```

Kích hoạt DRDB trên cả 2 node

```
modprobe drbd
```

Kiểm tra DRDB đã hoạt động hay chưa:

```
lsmod | grep drbd

drbd                  405309  0
libcrc32c              12644  2 xfs,drbd
```

Nếu tường lửa được kích hoạt hãy thêm rule cho phép DRDB:

```
firewall-cmd --permanent --add-port=7788/tcp
firewall-cmd --reload
```

### 2.5 Cấu hình DRDB <a name="2.5" />

Để cấu hình DRDB, chúng ta tạo một file có tên là `testdata1.res` với nội dung:

```
vi /etc/drbd.d/testdata1.res
```

```
resource testdata1 {
protocol C;        
on node1 {
                device /dev/drbd0;
                disk /dev/vgdrbd/vol1;
                address 192.168.100.196:7788;
                meta-disk internal;
        }
on node2 {
                device /dev/drbd0;
                disk /dev/vgdrbd/vol1;
                address 192.168.100.197:7788;
                meta-disk internal;
        }
} 
```

**Giải thích:**

- `resource testdata1`: Tên của resource
- `Protocol C`: Các resource được cấu hình để synchronous replication. Chi tiết <a href="http://www.learnitguide.net/2016/07/what-is-drbd-how-drbd-works-drbd.html">tại đây</a>
- `node1`, `node2`: Danh sách các node và các tùy chọn bên trong
- `device /dev/drbd0`: Xác định thiết bị logic được DRBD sử dụng (Nên đặt giống nhau ở trên 2 server)
- `disk /dev/vgdrbd/vol1`: Xác định thiết bị vật lý dùng để tạo ra thiết bị logic bên trên, và không nhất thiết phải cùng trên trên 2 server.
- `address 192.168.100.197:7788`: Xác định địa chỉ IP và Port của mỗi server
- `meta-disk internal`: Cho phép sử dụng Meta-data mức nội bộ

Sao chép file cấu hình sang node 2:

```
[root@node1 ~] scp /etc/drbd.d/testdata1.res node2:/etc/drbd.d/
```

Trên cả 2 node, chúng ta tạo một LVM để lưu trữ các dữ liệu mà chúng ta đã định nghĩa ở file cấu hình bên trên `/dev/vgdrbd/vol1`. Ở đây, `/dev/sdb` được sử dụng để tạo LVM

```
pvcreate /dev/sdb
  Physical volume "/dev/sdb" successfully created
vgcreate vgdrbd /dev/sdb
  Volume group "vgdrbd" successfully created
lvcreate -n vol1 -l100%FREE vgdrbd
  Logical volume "vol1" created.
```

### 2.6 Khởi động DRDB meta-data storage <a name="2.6" />

Sau khi cấu hình xong các bước bên trên, chúng ta chuyển tiếp đến phần khởi động thiết bị DRBD. Trên cả 2 node chúng ta thực hiện lần lượt các lệnh sau và đọc thông báo kết quả:

```
[root@node1 ~] drbdadm create-md testdata1
[root@node2 ~] drbdadm create-md testdata1

Above commands will give an output like below.
  --==  Thank you for participating in the global usage survey  ==--
The server's response is:
you are the 10680th user to install this version
initializing activity log
NOT initializing bitmap
Writing meta data...
New drbd meta data block successfully created.
success
```

Tiếp đến, chúng ta khởi động DRBD và kích hoạt khởi chạy cùng hệ thống:

```
systemctl start drbd
systemctl enable drbd
```

### 2.7 Xác định DRDB Primary node <a name="2.7" />

Xác định node DRBD Primary trên node1:

```
[root@node1 ~] drbdadm primary testdata1 --force
```

Kiểm tra kết quả quá trình đồng bộ:

```
[root@node1 ~]# cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by phil@Build64R7, 2016-01-12 14:29:40
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:1048508 nr:0 dw:0 dr:1049236 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
[root@node1 ~]# drbd-overview
 0:testdata1/0  Connected Primary/Secondary UpToDate/UpToDate
```

### 2.8 Tạo và mount filesystem thiết bị DRDB <a name="2.8" />

Làm bước này chỉ khi bước trên thành công, khi kiểm tra thấy DRBD báo `Connected`

```
[root@node1 ~]# drbd-overview
 0:testdata1/0  Connected Primary/Secondary UpToDate/UpToDate
```

Chúng ta tạo filesystem và ghi dữ liệu vào nó:

```
[root@node1 ~]# mkfs.ext3 /dev/drbd0
[root@node1 ~]# mount /dev/drbd0 /mnt
[root@node1 ~]# echo "Welcome to my Website" > /mnt/index.html
[root@node1 ~]# umount /mnt
```
